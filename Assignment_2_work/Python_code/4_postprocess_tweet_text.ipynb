{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here postprocess the tweet data that has been mapreduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['!\\t38425', '!!\\t4496', '!!!\\t3998', '!!!!\\t563', '!!!!!\\t222', '!!!!!!\\t162', '!!!!!!!\\t33', '!!!!!!!!\\t16', '!!!!!!!!!\\t13', '!!!!!!!!!!\\t60']\n"
     ]
    }
   ],
   "source": [
    "file_to_process = \"/Users/hannojacobs/MIT805_datasets/mapreduced_tweet_fields/raw_mapreduced_tweet_fields/Ukraine_mapreduced_6_text.txt\"\n",
    "\n",
    "lines_raw = []\n",
    "with open(file_to_process, 'r') as file:\n",
    "    for line in file:\n",
    "        lines_raw.append(line.strip())  # Use strip() to remove newline characters\n",
    "\n",
    "print(lines_raw[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove all non A-Z characters from the text and save it in a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '', '', '', '', '', '', '', '', '']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "\n",
    "    cleaned_text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    return cleaned_text\n",
    "\n",
    "with open(file_to_process, 'r', encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "    cleaned_text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "\n",
    "output_file_path = '/Users/hannojacobs/MIT805_datasets/mapreduced_tweet_fields/post_processed_tweet_fields/Ukraine_text_a-z_only.txt'\n",
    "\n",
    "with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
    "    output_file.write(cleaned_text)\n",
    "\n",
    "lines_a_z = []\n",
    "with open(output_file_path, 'r') as file:\n",
    "    for line in file:\n",
    "        lines_a_z.append(line.strip())  # Use strip() to remove newline characters\n",
    "\n",
    "print(lines_a_z[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now remove all newlines and spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text with empty lines and spaces removed saved to '/Users/hannojacobs/MIT805_datasets/mapreduced_tweet_fields/post_processed_tweet_fields/Ukraine_text_a-z_only.txt'.\n"
     ]
    }
   ],
   "source": [
    "file_path = output_file_path\n",
    "\n",
    "def remove_empty_lines_and_spaces(text):\n",
    "    lines = text.split('\\n')\n",
    "    cleaned_lines = [line.strip() for line in lines if line.strip()]\n",
    "    cleaned_text = '\\n'.join(cleaned_lines)\n",
    "    return cleaned_text\n",
    "\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "\n",
    "cleaned_text = remove_empty_lines_and_spaces(text)\n",
    "\n",
    "with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
    "    output_file.write(cleaned_text)\n",
    "\n",
    "print(f\"Text with empty lines and spaces removed saved to '{output_file_path}'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## remove all urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lines starting with 'https' removed and saved to '/Users/hannojacobs/MIT805_datasets/mapreduced_tweet_fields/post_processed_tweet_fields/Ukraine_text_a-z_only.txt'.\n"
     ]
    }
   ],
   "source": [
    "file_path = output_file_path\n",
    "\n",
    "def remove_lines_starting_with_https(text):\n",
    "    lines = text.split('\\n')\n",
    "    cleaned_lines = [line for line in lines if not line.strip().startswith('https')]\n",
    "    cleaned_text = '\\n'.join(cleaned_lines)\n",
    "    return cleaned_text\n",
    "\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "\n",
    "cleaned_text = remove_lines_starting_with_https(text)\n",
    "\n",
    "with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
    "    output_file.write(cleaned_text)\n",
    "\n",
    "print(f\"Lines starting with 'https' removed and saved to '{output_file_path}'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make all text small letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text converted to lowercase and saved to '/Users/hannojacobs/MIT805_datasets/mapreduced_tweet_fields/post_processed_tweet_fields/Ukraine_text_a-z_only.txt'.\n"
     ]
    }
   ],
   "source": [
    "file_path = output_file_path\n",
    "\n",
    "def convert_to_lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "\n",
    "lowercase_text = convert_to_lowercase(text)\n",
    "\n",
    "with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
    "    output_file.write(lowercase_text)\n",
    "\n",
    "print(f\"Text converted to lowercase and saved to '{output_file_path}'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now mapreduce again and then see what the most common words are"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take the new file and find the most common words by storing the file in a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = output_file_path = \"/Users/hannojacobs/MIT805_datasets/mapreduced_tweet_fields/post_processed_tweet_fields/out_Ukraine_text_a-z_only.txt\"\n",
    "\n",
    "word_frequencies_list = []\n",
    "with open(file_path, 'r') as file:\n",
    "    for line in file:\n",
    "        parts = re.split(r'\\s+', line.strip())\n",
    "        \n",
    "        if len(parts) >= 2:\n",
    "            word = parts[0]\n",
    "            frequency = int(parts[-1])\n",
    "            word_frequencies_list.append([word, frequency])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sort the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ukraine', 3098]\n",
      "['russia', 1422]\n",
      "['m', 1249]\n",
      "['putin', 1235]\n",
      "['war', 1191]\n",
      "['s', 1179]\n",
      "['k', 1131]\n",
      "['the', 1012]\n",
      "['you', 821]\n",
      "['standwithukraine', 793]\n",
      "['us', 793]\n",
      "['pm', 787]\n",
      "['this', 765]\n",
      "['now', 756]\n",
      "['day', 698]\n",
      "['a', 686]\n",
      "['b', 686]\n",
      "['it', 669]\n",
      "['nato', 628]\n",
      "['in', 599]\n",
      "['russian', 596]\n",
      "['th', 587]\n",
      "['peace', 569]\n",
      "['am', 553]\n",
      "['world', 540]\n",
      "['c', 536]\n",
      "['people', 527]\n",
      "['no', 511]\n",
      "['we', 507]\n",
      "['x', 468]\n",
      "['today', 445]\n",
      "['breaking', 437]\n",
      "['to', 418]\n",
      "['all', 411]\n",
      "['kyiv', 406]\n",
      "['news', 399]\n",
      "['i', 391]\n",
      "['russians', 384]\n",
      "['on', 373]\n",
      "['and', 365]\n",
      "['more', 365]\n",
      "['ukrainian', 361]\n",
      "['for', 356]\n",
      "['here', 356]\n",
      "['help', 344]\n",
      "['please', 344]\n",
      "['again', 343]\n",
      "['me', 339]\n",
      "['is', 338]\n",
      "['them', 331]\n",
      "['time', 325]\n",
      "['million', 321]\n",
      "['p', 319]\n",
      "['stop', 318]\n",
      "['that', 318]\n",
      "['ukrainians', 318]\n",
      "['km', 311]\n",
      "['children', 310]\n",
      "['china', 310]\n",
      "['why', 310]\n",
      "['country', 308]\n",
      "['up', 307]\n",
      "['europe', 305]\n",
      "['what', 305]\n",
      "['n', 303]\n",
      "['f', 302]\n",
      "['not', 299]\n",
      "['there', 299]\n",
      "['mariupol', 292]\n",
      "['its', 290]\n",
      "['do', 286]\n",
      "['slavaukraini', 286]\n",
      "['zelensky', 277]\n",
      "['if', 276]\n",
      "['one', 276]\n",
      "['eu', 273]\n",
      "['march', 271]\n",
      "['sanctions', 271]\n",
      "['years', 271]\n",
      "['president', 270]\n",
      "['trump', 270]\n",
      "['freedom', 267]\n",
      "['t', 267]\n",
      "['covid', 265]\n",
      "['ww', 265]\n",
      "['out', 262]\n",
      "['support', 260]\n",
      "['bn', 258]\n",
      "['love', 257]\n",
      "['video', 255]\n",
      "['like', 254]\n",
      "['nazis', 254]\n",
      "['usa', 253]\n",
      "['h', 252]\n",
      "['refugees', 252]\n",
      "['russias', 252]\n",
      "['days', 251]\n",
      "['putins', 251]\n",
      "['invasion', 249]\n",
      "['civilians', 247]\n"
     ]
    }
   ],
   "source": [
    "data = word_frequencies_list\n",
    "sorted_word_frequencies_list = sorted(data, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "for item in sorted_word_frequencies_list[0:100]:\n",
    "    print(item)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Throw away all items that are prepositions or not important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['ukraine', 3098], ['russia', 1422], ['putin', 1235], ['war', 1191], ['standwithukraine', 793], ['nato', 628], ['russian', 596], ['peace', 569], ['world', 540], ['people', 527], ['today', 445], ['breaking', 437], ['kyiv', 406], ['news', 399], ['russians', 384], ['ukrainian', 361], ['help', 344], ['please', 344], ['them', 331], ['stop', 318], ['ukrainians', 318], ['children', 310], ['china', 310], ['country', 308], ['europe', 305], ['mariupol', 292], ['slavaukraini', 286], ['zelensky', 277], ['march', 271], ['sanctions', 271], ['president', 270], ['trump', 270], ['freedom', 267], ['covid', 265], ['out', 262], ['support', 260], ['love', 257], ['video', 255], ['nazis', 254], ['usa', 253], ['refugees', 252], ['russias', 252], ['days', 251], ['putins', 251], ['invasion', 249], ['civilians', 247], ['watch', 246], ['poland', 244], ['west', 244], ['biden', 242], ['right', 236], ['kharkiv', 234], ['go', 233], ['amp', 232], ['new', 232], ['uk', 232], ['good', 231], ['live', 228], ['gt', 225], ['well', 225], ['india', 224], ['oil', 224], ['home', 218], ['army', 216], ['year', 213], ['yes', 213], ['are', 212], ['democracy', 212], ['history', 211], ['him', 210], ['end', 207], ['countries', 206], ['dont', 206], ['ukraines', 206], ['life', 205], ['russiaukraine', 202], ['truth', 200], ['friends', 198], ['humanity', 198], ['soon', 198], ['propaganda', 197], ['rt', 197], ['america', 196], ['man', 195], ['god', 194], ['moscow', 193], ['thread', 191], ['win', 191], ['un', 190], ['genocide', 189], ['media', 189], ['ukrainewar', 189], ['weapons', 189], ['free', 188], ['from', 188], ['women', 187], ['power', 186], ['soldiers', 185], ['ukrainerussiawar', 185], ['conflict', 184]]\n"
     ]
    }
   ],
   "source": [
    "word_list = sorted_word_frequencies_list\n",
    "\n",
    "# Words that are not needed and give no sentiment information\n",
    "stop_words = [\n",
    "    \"a\", \"an\", \"the\", \"in\", \"on\", \"at\", \"to\", \"for\", \"with\", \"by\", \"of\", \"about\",\n",
    "    \"as\", \"but\", \"or\", \"and\", \"is\", \"was\", \"were\", \"am\", \"be\", \"being\", \"been\",\n",
    "    \"have\", \"has\", \"had\", \"do\", \"does\", \"did\", \"can\", \"could\", \"will\", \"would\",\n",
    "    \"shall\", \"should\", \"may\", \"might\", \"must\", \"ought\", \"now\", \"here\", \"there\",\n",
    "    \"this\", \"that\", \"these\", \"those\", \"it\", \"he\", \"she\", \"they\", \"we\", \"you\", \"I\",\n",
    "    \"my\", \"mine\", \"our\", \"ours\", \"your\", \"yours\", \"his\", \"her\", \"hers\", \"its\",\n",
    "    \"their\", \"theirs\", \"any\", \"all\", \"some\", \"most\", \"no\", \"none\", \"neither\",\n",
    "    \"either\", \"each\", \"every\", \"other\", \"another\", \"such\", \"more\", \"most\",\n",
    "    \"few\", \"fewer\", \"little\", \"less\", \"many\", \"much\", \"several\", \"own\", \"same\",\n",
    "    \"so\", \"too\", \"very\", \"just\", \"now\", \"then\", \"there\", \"when\", \"where\", \"why\",\n",
    "    \"how\", \"while\", \"before\", \"after\", \"because\", \"since\", \"if\", \"unless\",\n",
    "    \"until\", \"although\", \"though\", \"even\", \"once\", \"since\", \"so\", \"than\", \"that\",\n",
    "    \"though\", \"till\", \"unless\", \"until\", \"when\", \"whenever\", \"where\", \"whereas\",\n",
    "    \"wherever\", \"whether\", \"while\", \"who\", \"whoever\", \"whose\", \"why\", \"although\",\n",
    "    \"because\", \"before\", \"since\", \"though\", \"until\", \"when\", \"while\", \n",
    "    \"us\", \"pm\", \"day\", \"th\", \"me\", \"time\", \"million\", \"again\", \"km\", \"up\", \"what\",\n",
    "    \"not\", \"one\", \"eu\", \"years\", \"ww\", \"bn\", \"like\",\n",
    "    \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\", \"j\", \"k\", \"l\", \"m\", \"n\",\n",
    "    \"o\", \"p\", \"q\", \"r\", \"s\", \"t\", \"u\", \"v\", \"w\", \"x\", \"y\", \"z\",\n",
    "]\n",
    "\n",
    "# Filter out stop words while retaining frequencies\n",
    "filtered_words_and_freq = [[word, freq] for word, freq in word_list if word not in stop_words]\n",
    "\n",
    "print(filtered_words_and_freq[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write this data to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been written to /Users/hannojacobs/MIT805_datasets/mapreduced_tweet_fields/post_processed_tweet_fields/ordered_text_words.txt\n"
     ]
    }
   ],
   "source": [
    "file_path = \"/Users/hannojacobs/MIT805_datasets/mapreduced_tweet_fields/post_processed_tweet_fields/ordered_text_words.txt\"\n",
    "\n",
    "with open(file_path, \"w\") as file:\n",
    "    for item in filtered_words_and_freq:\n",
    "        file.write(f\"{item[0]} {item[1]}\\n\")\n",
    "\n",
    "print(f\"Data has been written to {file_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
